{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unet-Tiles.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4mTqPjbyqVE"
      },
      "source": [
        "Caricamento del dataset da google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5GHmDhptRzb",
        "outputId": "dd0cdf3e-273d-42c4-f0f8-d1faac2d950d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "%cd /content/gdrive/My Drive/DataSet/DatasetTiles"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "/content/gdrive/My Drive/DataSet/DatasetTiles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYLsxFrwxWqn"
      },
      "source": [
        "LIbrerie principali da importare nel progetto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c91jzHPSuQTq"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset, Subset\n",
        "from torch import autograd, optim\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import cv2"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8rVGp1Sxd3C"
      },
      "source": [
        "**Costruzione della rete neurale U-Net**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAnfRKAUxtpH"
      },
      "source": [
        "Funzioni di utilit√† della rete"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_dWnWU5x2F1"
      },
      "source": [
        "def get_convolutional_layers(in_channel: int,\n",
        "                             out_channel: int,\n",
        "                             kernel_size: tuple = (3, 3),\n",
        "                             strides: tuple = (1, 1),\n",
        "                             padding: tuple = (1, 1)):\n",
        "    r\"\"\"\n",
        "    Convolutional layer.\n",
        "    :param in_channel: number of input filters.\n",
        "    :param out_channel: number of output filters.\n",
        "    :param kernel_size: kernel size.\n",
        "    :param strides: stride size.\n",
        "    :param padding: padding size.\n",
        "    \"\"\"\n",
        "\n",
        "    return nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=kernel_size, stride=strides,\n",
        "                     padding=padding)\n",
        "\n",
        "\n",
        "def get_relu():\n",
        "    r\"\"\"\n",
        "    Relu activation function.\n",
        "    \"\"\"\n",
        "    return nn.ReLU(inplace=True)\n",
        "\n",
        "\n",
        "def get_sigmoid():\n",
        "    r\"\"\"\n",
        "    Sigmoid activation function.\n",
        "    \"\"\"\n",
        "    return nn.Sigmoid()\n",
        "\n",
        "\n",
        "def get_max_pooling(kernel_size: tuple = (2, 2),\n",
        "                    strides: tuple = (2, 2)):\n",
        "    r\"\"\"\n",
        "    Max pooling layer.\n",
        "    :param kernel_size: kernel size.\n",
        "    :param strides: stride size.\n",
        "    \"\"\"\n",
        "    return nn.MaxPool2d(kernel_size=kernel_size, stride=strides)\n",
        "\n",
        "\n",
        "def get_up_sample(scale_factor: int = 2):\n",
        "    r\"\"\"\n",
        "    Up sample layer.\n",
        "    :param scale_factor: scale factor.\n",
        "    \"\"\"\n",
        "    return nn.Upsample(scale_factor=scale_factor)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AUpMCWXx-55"
      },
      "source": [
        "Blocchi della rete: blocco di down-sampling, bottleneck, blocco di up-sampling e di output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBDQo47CyFBY"
      },
      "source": [
        "class Down(nn.Module):\n",
        "    r\"\"\"\n",
        "    Downsampling block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channel: int,\n",
        "                 out_channel: int):\n",
        "        r\"\"\"\n",
        "        :param in_channel: number of input filters.\n",
        "        :param out_channel: number of output filters.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_1 = get_convolutional_layers(in_channel, out_channel)\n",
        "        self.conv_2 = get_convolutional_layers(out_channel, out_channel)\n",
        "        self.activation = get_relu()\n",
        "        self.max_pooling = get_max_pooling()\n",
        "\n",
        "    def forward(self, x):\n",
        "        r\"\"\"\n",
        "        Forward down block.\n",
        "        \"\"\"\n",
        "        out_conv = self.conv_1(x)\n",
        "        out_relu = self.activation(out_conv)\n",
        "\n",
        "        out_conv = self.conv_2(out_relu)\n",
        "        out_relu = self.activation(out_conv)\n",
        "\n",
        "        out_pooling = self.max_pooling(out_relu)\n",
        "        return out_relu, out_pooling\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    r\"\"\"\n",
        "    Bottleneck block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channel: int,\n",
        "                 out_channel: int):\n",
        "        r\"\"\"\n",
        "\n",
        "        :param in_channel: number of input filters.\n",
        "        :param out_channel: number of output filters.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_1 = get_convolutional_layers(in_channel, out_channel)\n",
        "        self.conv_2 = get_convolutional_layers(out_channel, out_channel)\n",
        "        self.activation = get_relu()\n",
        "\n",
        "    def forward(self, x):\n",
        "        r\"\"\"\n",
        "        Forward bottleneck block.\n",
        "        \"\"\"\n",
        "        out_conv = self.conv_1(x)\n",
        "        out_relu = self.activation(out_conv)\n",
        "\n",
        "        out_conv = self.conv_2(out_relu)\n",
        "        out_relu = self.activation(out_conv)\n",
        "\n",
        "        return out_relu\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    r\"\"\"\n",
        "    Upsampling block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channel: int,\n",
        "                 out_channel: int):\n",
        "        r\"\"\"\n",
        "        :param in_channel: number of input filters.\n",
        "        :param out_channel: number of output filters.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.up_sample = get_up_sample()\n",
        "        self.concat = Concat()\n",
        "        self.activation = get_relu()\n",
        "        self.conv_1 = get_convolutional_layers(in_channel=in_channel, out_channel=out_channel)\n",
        "        self.conv_2 = get_convolutional_layers(out_channel, out_channel)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        r\"\"\"\n",
        "        Forward upblock.\n",
        "        :param x: input layer.\n",
        "        :param skip: skip connection layer to concatenate.\n",
        "        \"\"\"\n",
        "        out_up = self.up_sample(x)\n",
        "        out_concat = self.concat(out_up, skip)\n",
        "        out_conv = self.conv_1(out_concat)\n",
        "        out_relu = self.activation(out_conv)\n",
        "        out_conv = self.conv_2(out_relu)\n",
        "        out_relu = self.activation(out_conv)\n",
        "\n",
        "        return out_relu\n",
        "\n",
        "\n",
        "class Concat(nn.Module):\n",
        "    r\"\"\"\n",
        "    Concatenate class.\n",
        "    Concatenate two levels of the network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, layer_1, layer_2):\n",
        "        out_concat = torch.cat((layer_1, layer_2), dim=1)\n",
        "        return out_concat\n",
        "\n",
        "\n",
        "class Out(nn.Module):\n",
        "    r\"\"\"\n",
        "    Output block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channel: int,\n",
        "                 out_channel: int,\n",
        "                 kernel_size: tuple):\n",
        "        r\"\"\"\n",
        "        :param in_channel: number of input filters.\n",
        "        :param out_channel: number of output filters.\n",
        "        :param kernel_size: kernel size.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = get_convolutional_layers(in_channel=in_channel, out_channel=out_channel, kernel_size=kernel_size,\n",
        "                                             padding=(0, 0))\n",
        "        self.sigmoid = get_sigmoid()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        r\"\"\"\n",
        "        Forward output block.\n",
        "        \"\"\"\n",
        "        out_conv = self.conv(x)\n",
        "        out_sigmoid = self.sigmoid(out_conv)\n",
        "\n",
        "        return out_sigmoid"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB8HyIVNyM7j"
      },
      "source": [
        "Corpo della rete"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLxVPVV0yOCS"
      },
      "source": [
        "class Unet(nn.Module):\n",
        "    r\"\"\"\n",
        "    U-net class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_classes_out: int,\n",
        "                 block_filter_count = [3, 64, 128, 256, 512, 1024]):\n",
        "        r\"\"\"\n",
        "        Builder of the class.\n",
        "        :param n_classes_out: number of classes of the problem.\n",
        "        :param block_filter_count: number of filters for each convolutional layer.\n",
        "        \"\"\"\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        self.blocks_down = []\n",
        "        self.blocks_up = []\n",
        "\n",
        "        # Down sampling path\n",
        "        print(\"** Block downs **\")\n",
        "        for i in range(0, 4):\n",
        "            block = Down(in_channel=block_filter_count[i], out_channel=block_filter_count[i + 1])\n",
        "            print(f\"Block down {i + 1}-layer: in: {block_filter_count[i]}, out: {block_filter_count[i + 1]}\")\n",
        "            self.blocks_down.append(block)\n",
        "\n",
        "        # Bottleneck\n",
        "        print(\"\\n** Bottleneck **\")\n",
        "        self.bottleneck = Bottleneck(in_channel=block_filter_count[4], out_channel=block_filter_count[5])\n",
        "        print(f\"Bottleneck layer: in: {block_filter_count[4]}, out: {block_filter_count[5]}\")\n",
        "\n",
        "        # Up sampling path\n",
        "        print(\"\\n** Block ups **\")\n",
        "        for i in range(0, 4):\n",
        "            block = Up(in_channel=block_filter_count[5 - i] + block_filter_count[5 - (i + 1)],\n",
        "                             out_channel=block_filter_count[5 - (i + 1)])\n",
        "            print(\n",
        "                f\"Block ups {i + 1}-layer: in: {block_filter_count[5 - i] + block_filter_count[5 - (i + 1)]}, out: {block_filter_count[5 - (i + 1)]}\")\n",
        "            self.blocks_up.append(block)\n",
        "\n",
        "        # Output layer\n",
        "        print(\"\\n** Out net **\")\n",
        "\n",
        "        self.out = Out(in_channel=block_filter_count[1], out_channel=n_classes_out, kernel_size=(1, 1))\n",
        "        print(f\"Out layer: in: {block_filter_count[1]}, out: {n_classes_out}\\n\")\n",
        "\n",
        "        self.blocks_down = nn.ModuleList(self.blocks_down)\n",
        "        self.bottleneck = nn.ModuleList([self.bottleneck])\n",
        "        self.blocks_up = nn.ModuleList(self.blocks_up)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        r\"\"\"\n",
        "        Forward of the network.\n",
        "        :param x: input net.\n",
        "        :return: output net.\n",
        "        \"\"\"\n",
        "\n",
        "        encoder = []\n",
        "\n",
        "        # Down sampling path\n",
        "        for block in self.blocks_down:\n",
        "            out_conv, out_pool = block(x)\n",
        "\n",
        "            encoder.append(out_conv)\n",
        "            x = out_pool\n",
        "\n",
        "        out_down_blocks = x\n",
        "\n",
        "        # Bottleneck\n",
        "        bn = self.bottleneck[0](out_down_blocks)\n",
        "        x = bn\n",
        "\n",
        "        # Up sampling path\n",
        "        for index, block in enumerate(self.blocks_up):\n",
        "            x = block(x, encoder[len(encoder) - (index + 1)])\n",
        "\n",
        "        # Output layer\n",
        "        out_net = self.out(x)\n",
        "\n",
        "        return out_net\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2arh5hh7zgZ3"
      },
      "source": [
        "Classe per caricamento del dataset nella rete con successiva pre-elaborazione delle immagini"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlpaZ0i-yI7c"
      },
      "source": [
        "WIDTH = 256\n",
        "HEIGHT = 256\n",
        "\n",
        "\n",
        "class DatasetTiles(Dataset):\n",
        "    r\"\"\"\n",
        "    Class to load the dataset of a specific defect.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, parent_dir, image_dir):\n",
        "        r\"\"\"\n",
        "        Load the dataset.\n",
        "        :param parent_dir: root folder.\n",
        "        :param image_dir: directory of the defect.\n",
        "\n",
        "        Image format:\n",
        "            - .jpg: image\n",
        "            - .png: binay mask\n",
        "        \"\"\"\n",
        "        self.img_list_path = glob.glob(parent_dir + '/' + image_dir + '/Imgs/*.jpg')\n",
        "        self.img_mask_list_path = glob.glob(parent_dir + '/' + image_dir + '/Imgs/*.png')\n",
        "        print(f\"{image_dir} loaded!\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        r\"\"\"\n",
        "        Get the image and its mask\n",
        "        :param index: index of the specific image\n",
        "        \"\"\"\n",
        "\n",
        "        x = preprocessing(self.img_list_path[index], False)\n",
        "        # Resize input format [height, width, n_channels]\n",
        "        x = np.rollaxis(x, 2, 0)\n",
        "\n",
        "        y = preprocessing(self.img_mask_list_path[index], True)\n",
        "        # Add 1 channel to input. Input format [height, width, n_channels]\n",
        "        y = np.expand_dims(y, axis=0)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list_path)\n",
        "\n",
        "\n",
        "def preprocessing(img, convert_to_gray):\n",
        "    r\"\"\"\n",
        "    Performs a preprocessing on the image.\n",
        "    :param img: img to be processed\n",
        "    :param convert_to_gray: true if the conversion is grayscale, false otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    img = cv.imread(img)\n",
        "    img = cv.resize(img, (WIDTH, HEIGHT))\n",
        "\n",
        "    if convert_to_gray:\n",
        "        img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "    else:\n",
        "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "\n",
        "    img = img / 255\n",
        "    img = img.astype(np.float32)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def train_test_split(dataset):\n",
        "    r\"\"\"\n",
        "    Slip dataset in training, validation and test set:\n",
        "        - 70% training set;\n",
        "        - 20% validation set:\n",
        "        - 10% test set.\n",
        "\n",
        "    :param dataset: dataset to split\n",
        "    :return : training, validation and test set.\n",
        "    \"\"\"\n",
        "    length_dataset = len(dataset)\n",
        "\n",
        "    length_train = np.int_(length_dataset * 0.7)\n",
        "    length_validate = np.int_(length_dataset * 0.2)\n",
        "\n",
        "    training_dataset = Subset(dataset, range(0, length_train))\n",
        "    validation_dataset = Subset(dataset, range(length_train, length_train + length_validate))\n",
        "    test_dataset = Subset(dataset, range(length_train + length_validate, len(dataset)))\n",
        "\n",
        "    return training_dataset, validation_dataset, test_dataset\n"
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}